{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. tensorflow hello world\n",
    "\n",
    "最简单的神经网络、一层、一个单元、一个输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
    "ys = np.array([-2.0, 1.0, 4.0, 7.0, 10.0, 13.0], dtype=float)\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "# 添加全连接层\n",
    "model.add(keras.layers.Dense(units=1, input_shape=[1]))\n",
    "\n",
    "# 指定损失函数 和 优化器\n",
    "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
    "\n",
    "# 训练\n",
    "model.fit(xs, ys, epochs=100)\n",
    "\n",
    "# 预测\n",
    "model.predict([10.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 手写数字识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data len: 60000\n",
      "testing data len: 10000\n"
     ]
    }
   ],
   "source": [
    "print(f'training data len: {len(x_train)}')\n",
    "print(f'testing data len: {len(x_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是做了数据归一化处理\n",
    "\n",
    "数据的标准化（normalization）是将数据按比例缩放，使之落入一个小的特定区间。\n",
    "数据标准化最典型的就是数据的归一化处理，即将数据统一映射到[0,1]区间上。从经验上说，归一化是让不同维度之间的特征在数值上有一定比较性，可以大大提高分类器的准确性。\n",
    "\n",
    "归一化好处：\n",
    "1. 提升模型的收敛速度\n",
    "2. 提升模型的精度\n",
    "\n",
    "如果多了这一步，准确率达到97.6%， 少了这步准确率不到95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, x_test = x_train / 255.0, x_test / 255.0  # 数据归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABAElEQVR4nGNgGMyAWUhIqK5jvdSy/9/rGRgYGFhgEnJsVjYCwQwMDAxPJgV+vniQgYGBgREqZ7iXH8r6l/SV4dn7m8gmCt3++/fv37/Htn3/iMW+gDnZf/+e5WbQnoXNNXyMs/5GoQoxwVmf/n9kSGFiwAW49/11wynJoPzx4YIcRlyygR/+/i2XxCWru+vv32nSuGQFYv/83Y3b4p9/fzpAmSyoMnohpiwM1w5h06Q+5enfv39/bcMiJVF09+/fv39P+mFKiTtd/fv3799jgZiBJLT69t+/f/8eDuDEkDJf8+jv379/v7Ryo4qzMDAwMAQGMjBc3/y35wM2V1IfAABFF16Aa0wAOwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x17B1845F8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取一张图来查看下\n",
    "# 每一张图都是28 * 28像素的二值图像\n",
    "print(y_train[0])\n",
    "Image.fromarray(x_train[0]).convert('L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.0313 - accuracy: 0.9894\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 3s 52us/sample - loss: 0.0285 - accuracy: 0.9905\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 3s 52us/sample - loss: 0.0287 - accuracy: 0.9901\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 3s 52us/sample - loss: 0.0281 - accuracy: 0.9907\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 3s 51us/sample - loss: 0.0252 - accuracy: 0.9916\n",
      "10000/10000 - 0s - loss: 0.0851 - accuracy: 0.9806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08514741737058502, 0.9806]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. TensorFlow实现CNN，对手写数字进行识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Reshape\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "# 定义输入层\n",
    "model.add(Reshape((28, 28, 1), input_shape=(28, 28)))\n",
    "\n",
    "# 添加一层卷积层, 卷积层设置多少个filters就会有多少个输出\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "# 添加一层池化层\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 再添加一层卷积层\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "# 再添加一层池化层\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 不理解为什么后面的卷积层可以增减filters？？？\n",
    "# 比如可以再添加一层卷积层，但filters可以设置64\n",
    "# model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'max_pooling2d_15/Identity:0' shape=(None, 5, 5, 32) dtype=float32>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output\n",
    "\n",
    "# 可以看到 输出的shape是(None, 7, 7, 32)， 其中None 是batch_size，有32个7 * 7 size的矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行flatten\n",
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'flatten_6/Identity:0' shape=(None, 800) dtype=float32>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output\n",
    "\n",
    "# flatten 后就变成了一维的 size为1568"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加全连接层\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "\n",
    "# 再添加一层全连接层作为输出层\n",
    "model.add(Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_13/Identity:0' shape=(None, 10) dtype=float32>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_10 (Reshape)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 11, 11, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 32)                25632     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 35,530\n",
      "Trainable params: 35,530\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "\n",
    "# 可以看出，采用默认的padding的时候，每进行一次卷积操作，特征图像大小就会减少2（如果采用padding='same'，则图片大小不变）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置模型训练时的优化器、损失函数，这里我们采用adam作为优化器，损失函数为交叉熵。\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 18s 302us/sample - loss: 0.0213 - accuracy: 0.9935\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 18s 304us/sample - loss: 0.0146 - accuracy: 0.9956\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 19s 309us/sample - loss: 0.0132 - accuracy: 0.9956\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 22s 371us/sample - loss: 0.0135 - accuracy: 0.9955\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 20s 330us/sample - loss: 0.0141 - accuracy: 0.9952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14911d438>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=128, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 125us/sample - loss: 0.0498 - accuracy: 0.9879\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.049755994082004464, 0.9879]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
